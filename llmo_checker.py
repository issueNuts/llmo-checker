import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import date
import time

# --- Streamlit UI ---
st.set_page_config(page_title="LLMOãƒã‚§ãƒƒã‚¯ v0.1b", layout="wide")
st.title("ğŸ” LLMOãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ« v0.1b")
st.markdown("æŒ‡å®šãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§Googleã‚„AIæ¤œç´¢ã«ã©ã‚Œãã‚‰ã„å‡ºã¦ãã‚‹ã‹ã‚’èª¿æŸ»ãƒ»å¯è¦–åŒ–ã—ã¾ã™ã€‚")

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
with st.form("llmo_form"):
    domain = st.text_input("èª¿æŸ»å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆä¾‹: xxxxx.co.jpï¼‰", "")
    keywords_input = st.text_area("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆ1è¡Œã«1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€æœ€å¤§10ä»¶ï¼‰", "")
    enable_debug = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆè§£æä¸­ã®URLã‚’è¡¨ç¤ºï¼‰", value=False)
    submitted = st.form_submit_button("åˆ†æã‚¹ã‚¿ãƒ¼ãƒˆ")

# --- å‡¦ç† ---
if submitted and domain and keywords_input:
    keywords = [k.strip() for k in keywords_input.strip().split("\n") if k.strip()][:10]
    result_data = []

    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    for kw in keywords:
        query = kw.replace(" ", "+")
        url = f"https://www.google.com/search?q={query}"
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")
        results = soup.select("div.tF2Cxc")

        found = False
        match_url = ""
        snippet = ""
        debug_urls = []

        for r in results[:5]:
            link_tag = r.find("a", href=True)
            if link_tag:
                link = link_tag["href"]
                text = r.get_text()
                debug_urls.append(link)
                # ã‚†ã‚‹ã„ä¸€è‡´åˆ¤å®š
                if domain in link or f"www.{domain}" in link or domain in text:
                    found = True
                    match_url = link
                    snippet = text[:150]
                    break

        result_data.append({
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": kw,
            "ãƒ’ãƒƒãƒˆ": "â—‹" if found else "Ã—",
            "URL": match_url,
            "æŠœç²‹": snippet,
            "ãƒ‡ãƒãƒƒã‚°URLä¸€è¦§": "; ".join(debug_urls) if enable_debug else ""
        })

        time.sleep(1.5)  # è² è·å¯¾ç­–

    df = pd.DataFrame(result_data)
    st.success(f"æ¤œç´¢å®Œäº†ï¼{len(df)} ä»¶ä¸­ {df['ãƒ’ãƒƒãƒˆ'].tolist().count('â—‹')} ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
    st.dataframe(df)

    # ãƒ’ãƒƒãƒˆç‡ã®é›†è¨ˆ
    hit_rate = df["ãƒ’ãƒƒãƒˆ"].tolist().count("â—‹") / len(df) * 100
    st.metric("ãƒ’ãƒƒãƒˆç‡", f"{hit_rate:.1f}%")

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button("CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name=f"llmo_results_{date.today()}.csv", mime="text/csv")

    # TODO: AIæ¤œç´¢é€£æºï¼ˆPerplexity APIãªã©ï¼‰ã‚’æ¬¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§è¿½åŠ 
    st.info("â€» æ¬¡å›ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§Perplexityï¼ˆAIæ¤œç´¢ï¼‰ã«ã‚‚å¯¾å¿œäºˆå®šã§ã™ã€‚")

else:
    st.info("ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€åˆ†æã‚¹ã‚¿ãƒ¼ãƒˆã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
